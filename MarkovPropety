                                                           The Markov Property
In the reinforcement learning framework, the agent makes its decisions as a function of a signal from the environment called the environment's state.
In this section we discuss what is required of the state signal, and what kind of information we should and should not expect it to provide.In particular, we formally define a property of environmets and their state signals that is of particular interest, called the Markov property.
Certainly the state signal should include immediate sensations such as sensory measurements, but it can contain much more that that. State representations can be highly processed versions of original sensations, or they can be complex structures built up over time from the sequence of sensations. For example, we can move our eyes over a scene, with only a tiny spot corresponding to the fovea visible in detail at any one time, yet build up a rich and detailed representation of a scene. Or, more obviously, we can look at an object, then look away and know that it is still there. We can hear the word "yes" and consider ourselves to be in totally different states depending on the question that came before and which is no longer audible. At a more mundane level, a control system can measure position at two different times to produce a state representation including information about velocity.IN ALL OF THESE CASES THE STATE IS CONSTRUCTED AND MAINTAINED ON THE BASIS OF IMMEDIATE SENSATIONS TOGETHER WITH THE PREVIOUS STATE OR SOME OTHER MEMORY OF PAST SENSATIONS.

What we would like, ideally, is a state signal that summarizes past sensations compactly, yet in such a way that all relevant information is retained.This normally requires more than the immediate sensations, but never more than the complete history of all past sensations. A STATE SIGNAL THAT SUCCEEDS IN RETAINING ALL RELEVANT INFORMATION IS SAID TO BE IN MARKOV, OR TO HAVE THE MARKOV PROPERTY.

For example, the current position and velocity of a cannonball is all that matters for its future flight. It doesn't matter how that position and velocity came about.

The Markov property is important in reinforcement learning because decisions and values are assumed to be a function only of the current state. In order for these to be effective and informative, the state representation must be informative.All of theory presented in this book assumes Markov state signals. This means that not all the theory strictly applies to cases in which the Markov property does not strictly apply.
However, the theory developed for the Markov case still helps us to understand the behavior of the algorithms, and the algorithms can be successfully applied to many tasks with state that are not strictly Markov.
A full understanding of the theory of the Markov case is an essencial foundation for extending it to the more complex and realistic non-Markov case. Finally, we note that the assumption of Markov state representations
is not all other approaches to artificial intelligience.
